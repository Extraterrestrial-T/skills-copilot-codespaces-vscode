{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-12T23:46:08.772739Z","iopub.execute_input":"2023-08-12T23:46:08.773072Z","iopub.status.idle":"2023-08-12T23:46:08.830423Z","shell.execute_reply.started":"2023-08-12T23:46:08.773039Z","shell.execute_reply":"2023-08-12T23:46:08.829541Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/biomass/Biomass_History.csv\n/kaggle/input/distance/Distance_Matrix.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install keras-rl2\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T23:46:23.863353Z","iopub.execute_input":"2023-08-12T23:46:23.863719Z","iopub.status.idle":"2023-08-12T23:46:37.067603Z","shell.execute_reply.started":"2023-08-12T23:46:23.863685Z","shell.execute_reply":"2023-08-12T23:46:37.066455Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting keras-rl2\n  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (from keras-rl2) (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (3.9.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (16.0.0)\nRequirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-rl2) (0.31.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-rl2) (0.2.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-rl2) (1.11.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.4.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.3.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow->keras-rl2) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.2.2)\nInstalling collected packages: keras-rl2\nSuccessfully installed keras-rl2-1.0.5\n","output_type":"stream"}]},{"cell_type":"code","source":"import plotly.express as px\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import MeanSquaredLogarithmicError,MeanSquaredError,MeanAbsoluteError\nfrom matplotlib import pyplot as plt\nfrom shapely.geometry import Point, MultiPoint\nimport rtree\nfrom sklearn.neighbors import NearestNeighbors\nfrom gym import Env\nfrom gym.spaces import Box, MultiDiscrete, Dict,Discrete\nfrom shapely.ops import nearest_points\nimport geopandas as gpd\nimport seaborn as sns\nimport random\nfrom rl.agents import DQNAgent\nfrom rl.policy import BoltzmannQPolicy\nfrom rl.memory import SequentialMemory\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-08-12T23:46:39.788827Z","iopub.execute_input":"2023-08-12T23:46:39.789275Z","iopub.status.idle":"2023-08-12T23:46:51.256392Z","shell.execute_reply.started":"2023-08-12T23:46:39.789237Z","shell.execute_reply":"2023-08-12T23:46:51.255391Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"biomass = pd.read_csv(\"/kaggle/input/biomass/Biomass_History.csv\")\ndistance = pd.read_csv(\"/kaggle/input/distance/Distance_Matrix.csv\")\n# creating point objects, will be useful later on \npoints = []\nfor i, row in biomass.iterrows():\n    points.append((float(row['Longitude']),float(row[\"Latitude\"])) )\n\nbiomass[\"Geometries\"] = points\nbiomass[\"Type\"] = \"Biomass prod.plants\"\nprint(biomass.head())\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T23:46:59.943011Z","iopub.execute_input":"2023-08-12T23:46:59.943831Z","iopub.status.idle":"2023-08-12T23:47:02.216718Z","shell.execute_reply.started":"2023-08-12T23:46:59.943791Z","shell.execute_reply":"2023-08-12T23:47:02.214592Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"   Index  Latitude  Longitude       2010       2011       2012       2013  \\\n0      0  24.66818   71.33144   8.475744   8.868568   9.202181   6.023070   \n1      1  24.66818   71.41106  24.029778  28.551348  25.866415  21.634459   \n2      2  24.66818   71.49069  44.831635  66.111168  56.982258  53.003735   \n3      3  24.66818   71.57031  59.974419  80.821304  78.956543  63.160561   \n4      4  24.66818   71.64994  14.653370  19.327524  21.928144  17.899586   \n\n        2014       2015       2016        2017            Geometries  \\\n0  10.788374   6.647325   7.387925    5.180296  (71.33144, 24.66818)   \n1  34.419411  27.361908  40.431847   42.126945  (71.41106, 24.66818)   \n2  70.917908  42.517117  59.181629   73.203232  (71.49069, 24.66818)   \n3  93.513924  70.203171  74.536720  101.067352  (71.57031, 24.66818)   \n4  19.534035  19.165791  16.531315   26.086885  (71.64994, 24.66818)   \n\n                  Type  \n0  Biomass prod.plants  \n1  Biomass prod.plants  \n2  Biomass prod.plants  \n3  Biomass prod.plants  \n4  Biomass prod.plants  \n","output_type":"stream"}]},{"cell_type":"code","source":"distance.iloc[1,19]\n","metadata":{"execution":{"iopub.status.busy":"2023-08-07T06:32:31.580742Z","iopub.execute_input":"2023-08-07T06:32:31.581151Z","iopub.status.idle":"2023-08-07T06:32:31.588881Z","shell.execute_reply.started":"2023-08-07T06:32:31.581121Z","shell.execute_reply":"2023-08-07T06:32:31.587765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(biomass.corr(), annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T11:17:30.217685Z","iopub.execute_input":"2023-08-05T11:17:30.218035Z","iopub.status.idle":"2023-08-05T11:17:30.728367Z","shell.execute_reply.started":"2023-08-05T11:17:30.218009Z","shell.execute_reply":"2023-08-05T11:17:30.726590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter_mapbox(biomass, lat=\"Latitude\", lon=\"Longitude\",hover_data=[\"Index\"],\n                        color=\"2014\", zoom=3, height=400)\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show(renderer='iframe')","metadata":{"execution":{"iopub.status.busy":"2023-08-08T03:11:11.714978Z","iopub.execute_input":"2023-08-08T03:11:11.715432Z","iopub.status.idle":"2023-08-08T03:11:13.569343Z","shell.execute_reply.started":"2023-08-08T03:11:11.715391Z","shell.execute_reply":"2023-08-08T03:11:13.568131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predictions using neural networks\n\n\"\"\" First a little data prep. We need to batch the results from 2010 till 2016, the 2017 \nresults will serve as the target values for the neural network\"\"\"\n\nx = biomass[[str(i+2000) for i in range(10,17)]].values\nprint(x.shape)\ny = biomass[\"2017\"].values\nprint(y.shape)\nscaler = StandardScaler()\nx = scaler.fit_transform(x)\nX_train,X_test,Y_train,Y_test = train_test_split(x,y, test_size = 0.25, random_state = 32)\nprint (X_train.shape, X_test.shape)\n\n\ndef build_net_no_dropout(input_size):\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Dense(input_size,activation = 'relu'))\n    model.add(tf.keras.layers.Dense(128, activation = \"relu\"))\n    model.add(tf.keras.layers.Dense(32, activation = \"relu\"))\n    model.add(tf.keras.layers.Dense(1, activation = 'relu'))\n    return model\n\n\ndef build_net_with_dropout(input_size):\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Dense(input_size,activation = 'relu',kernel_regularizer = tf.keras.regularizers.L1(0.01)))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(128, activation = \"relu\",kernel_regularizer = tf.keras.regularizers.L1(0.01)))\n    model.add(tf.keras.layers.Dropout(0.7))\n    model.add(tf.keras.layers.Dense(32, activation = \"relu\",kernel_regularizer = tf.keras.regularizers.L1(0.01)))\n    model.add(tf.keras.layers.Dense(1, activation = 'relu'))\n    return model\n\ndef model_run(model,train_x,val_y, train_y, val_x, num_epochs):\n    loss = MeanSquaredLogarithmicError()\n    optimizer = Adam(learning_rate = 0.005)\n    model.compile( loss= loss, optimizer = optimizer, metrics = [\"msle\"])\n    #training\n    history = model.fit(train_x,train_y, epochs = num_epochs,batch_size= 70, verbose = 1,validation_data = (val_x,val_y))\n    return history\n\ndef plotter(history,metric):\n    plt.plot(history.history[metric])\n    plt.plot(history.history['val_'+metric])\n    plt.xlabel('Epochs')\n    plt.ylabel(metric)\n    plt.legend([metric,'val_'+metric])\n    plt.show()\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:34:35.287702Z","iopub.execute_input":"2023-08-04T02:34:35.288087Z","iopub.status.idle":"2023-08-04T02:34:35.308723Z","shell.execute_reply.started":"2023-08-04T02:34:35.288058Z","shell.execute_reply":"2023-08-04T02:34:35.307600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#putting it all together\n\nann_without = build_net_no_dropout(X_train.shape[1])\nhist_ann_without = model_run(ann_without,X_train,Y_test,Y_train,X_test,30)\nplotter(hist_ann_without, \"msle\")\nplotter(hist_ann_without, \"loss\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:34:44.447453Z","iopub.execute_input":"2023-08-04T02:34:44.448605Z","iopub.status.idle":"2023-08-04T02:34:49.999212Z","shell.execute_reply.started":"2023-08-04T02:34:44.448533Z","shell.execute_reply":"2023-08-04T02:34:49.998068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann_with = build_net_with_dropout(X_train.shape[1])\nhist_ann_with = model_run(ann_with,X_train,Y_test,Y_train,X_test,40)\nplotter(hist_ann_with, \"msle\")\nplotter(hist_ann_with, \"loss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nPredictions = ann_without.predict(X_test)\nReal = Y_test\ndft = pd.DataFrame({\"real_2017\":list(Y_test),\"Prediction_2017\":list(Predictions)})\nplt.figure(figsize =(20,4))\nplt.plot(dft[\"real_2017\"])\nplt.plot(dft[\"Prediction_2017\"])\nplt.legend([\"Real\", \"Predictions\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-04T02:36:45.207842Z","iopub.execute_input":"2023-08-04T02:36:45.208257Z","iopub.status.idle":"2023-08-04T02:36:45.642100Z","shell.execute_reply.started":"2023-08-04T02:36:45.208222Z","shell.execute_reply":"2023-08-04T02:36:45.640906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# each additional component in the waste to energy supply chain will be represented \n\nclass Component():\n    def __init__(self, index, name, num, cap, points,k, pointx,data=biomass.copy(),distance = distance.copy(),refinery=False,spare =points.copy()):\n        self.name = name\n        self.index = index\n        self.num = num       \n        self.capacity = cap\n        self.tree = rtree.index.Rtree()\n        if refinery==True:\n            self.point = spare.copy()[index]\n            \n        else:\n            self.point = points[index]\n            del(spare)\n        for i,p in enumerate(pointx):\n            self.tree.insert(i,p+p,p)\n       \n            \n        \n        # k nearest point\n        self.sources= list(self.tree.nearest(self.point, k,objects='raw' ))\n        #print(len(self.sources))\n        self.sources_index = []\n        \n        for i in self.sources:\n            if refinery ==False:\n                self.sources_index.append(points.index(i))\n                pointx.remove(i)\n            elif refinery == True:\n                self.sources_index.append(spare.index(i))\n                pointx.remove(i)\n                \n        self.sources= self.sources[1:]\n        self.sources_index= self.sources_index[1:]\n        #print(\"Original number of sources {}\".format(len(self.sources)))\n        \n        #this part calculates the total output of the sources  \n        sources_output = data[\"2017\"][self.sources_index].sum()\n        \n        self.under_util = 0\n        if sources_output>self.capacity:\n            while True:\n                pointx.append(self.sources[-1])\n                self.sources.pop()\n                self.sources_index.pop()\n                sources_output = data[\"2017\"][self.sources_index].sum()\n                \n                if sources_output<=self.capacity:\n                    self.under_util = self.capacity-sources_output\n                    break\n                else:\n                    continue\n           \n        self.total_dist = 0\n        for i in self.sources_index:\n            self.total_dist+=distance.iloc[i,self.index]\n        self.pointx = pointx[:]\n        #print(\"final number of sources {}\".format(len(self.sources)))\n        \n    def prevent_duplicates(self):\n        return self.pointx\n            \n    def add(self,df):\n        q= int(self.index)\n        df.Type[q] = str(self.name)\n        return df\n    def cost(self):\n        return self.total_dist,self.under_util\n        \n\n\n\ndef make_components(number,name, k, cap,pointlist,pointx=[],index_=-1,refinery=False):\n   \n    components = [] \n    if 1==1:        \n        listr = [] \n        pointx = pointlist[:]\n        for i in range(number):\n            index = random.randint(0,2417)\n            if index not in listr :\n                \n                \n                comp = Component(index,name,i,cap,pointlist,k,pointx,refinery = refinery)\n                pointx= comp.prevent_duplicates()\n                components.append(comp)\n                listr.append(index)\n                \n            else:\n                i = i-1\n                continue\n    print(\"done\")\n    return components,pointx\n        \n    \n    \n     \n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T23:48:19.613587Z","iopub.execute_input":"2023-08-12T23:48:19.614136Z","iopub.status.idle":"2023-08-12T23:48:19.665855Z","shell.execute_reply.started":"2023-08-12T23:48:19.614091Z","shell.execute_reply":"2023-08-12T23:48:19.664937Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"refs=Component(90,\"depo_location\",90,2000,points,3,points[2:2220],refinery = False)\nprint(refs.index)","metadata":{"execution":{"iopub.status.busy":"2023-08-12T18:57:35.829484Z","iopub.execute_input":"2023-08-12T18:57:35.830170Z","iopub.status.idle":"2023-08-12T18:57:35.979992Z","shell.execute_reply.started":"2023-08-12T18:57:35.830125Z","shell.execute_reply":"2023-08-12T18:57:35.978794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating a custom environment with gym\n\nclass Environment(Env):\n    def __init__(self,num_dep,num_refinery, points = points):\n        a = 0.001\n        b = 1\n        c = 1\n        self.k = 80\n        self.points = points \n        self.num_dep = num_dep\n        self.refinery_cap = 100000\n        self.num_refinery = num_refinery\n        self.dep_cap = 20000\n        self.depots,self.pointx = make_components(num_dep,\"depot_location\",self.k, 20000,points)\n        self.dep_points = [q.point for q in self.depots]\n        self.refineries,self.refpointx = make_components(self.num_refinery,\"refinery_location\",int(num_dep//num_refinery), 100000,self.dep_points,refinery=True)\n        self.ref_points = [q.point for q in self.refineries]\n        self.transport = 0\n        self.underutil = 0\n        self.ndim=(self.num_dep+self.num_refinery,2)\n        self.observation_space = Box(high= np.inf,low = -np.inf,shape = (1,))\n        self.action_space =Discrete(np.prod(self.ndim))\n        for i in self.depots:\n            tr,un = i.cost()\n            self.transport+=tr\n            self.underutil+=un\n        for i in self.refineries:\n            tr,un = i.cost()\n            self.transport+=tr\n            self.underutil+=un\n        self.state = (a*self.transport)+(c*self.underutil)\n        self.episode_len = 80\n        self.previous_score = []\n        self.reinitialize= {\"depots\":self.depots,\n                            \"refinery\":self.refineries, \n                            \"refinery_points\":self.ref_points,\n                            \"depot_points\" =self.dep_points,\n                            \"deppointx\":self.pointx,\n                            'refpoints':self.refpointx,\n                            \n                           }\n            \n    def observe(self):\n        return np.array([self.state])\n    \n    def reset(self):\n        [\"depots\"]:self.depots,\n        [\"refinery\"] = self.refineries, \n        [\"refinery_points\"] =self.ref_points,\n        [\"depot_points\"] =self.dep_points,\n        [\"deppointx\"] = self.pointx,\n        ['refpoints'] = self.refpointx\n        self.previous_score = []\n        self.recalc()\n        obs = self.observe()\n        return obs\n        \n    def recalc (self):\n        a = 0.001\n        b = 1\n        c = 1\n        for i in self.depots:\n            tr,un = i.cost()\n            self.transport+=tr\n            self.underutil+=un\n        for i in self.refineries:\n            tr,un = i.cost()\n            self.transport+=tr\n            self.underutil+=un\n        var = (a*self.transport)+(c*self.underutil)\n        return var\n        \n    def close(self):\n        pass\n        \n    def step(self, action):\n        reward = 0\n        mapping = tuple(np.ndindex(self.ndim))\n        action = mapping[action]\n        if action[0]>=(self.num_dep) and self.episode_len>0:\n            # if the agent picks from [0, self.num_dep] that indicates a depot, else its a refinery\n            # in this case it is a refinery that has been selected\n            self.episode_len -= 1 \n              #selcting one the final motion [left:0,right:1]                \n            rp = self.ref_points[action[0]-self.num_dep]\n            indx = self.points.index(rp)\n            if action[1] == 0: #left\n                if indx-10>=0:\n                    for i in self.refineries[action[0]-self.num_dep].sources:\n                        self.refpointx.append(i)\n                        \n                        \n                    ref_new =Component(indx-10,\"refinery_location\",indx-10,100000,self.dep_points,4,self.refpointx,refinery = True)\n                    self.refpointx = ref_new.prevent_duplicates()\n                    self.refineries[action[0]-self.num_dep] = ref_new\n                    self.ref_points[action[0]-self.num_dep] = ref_new.point \n                    self.state = self.recalc() \n                else:\n                    reward -= 20\n                    \n            elif action[1] == 1:# right\n                if indx+10<=2417:\n                    for i in self.refineries[action[0]-self.num_dep].sources:\n                        self.refpointx.append(i)\n                    ref_new=Component(indx+10,\"refinery_location\",indx+10,100000,self.dep_points,4,self.refpointx,refinery = True)                     \n                    self.refpointx = ref_new.prevent_duplicates()\n                    print(type(ref_new))\n                    self.refineries[action[0]-self.num_dep] =ref_new\n                   \n                    self.ref_points[action[0]-self.num_dep] =ref_new.point \n                    self.state = self.recalc()\n                else:\n                    reward -= 20      \n            \n        elif action[0]<(self.num_dep) and self.episode_len>0:                               #in this case the agent selects a depot\n            self.episode_len -= 1 \n              #selcting one the final motion [left,right]\n            dp = self.dep_points[action[0]]\n            indx = self.points.index(dp)\n            if action[1] == 0: #left\n                \n                if indx-10>=0:\n                    for i in self.depots[action[0]].sources:\n                        self.pointx.append(i)\n                    dep_new= Component(indx-10,\"depot_location\",indx-10,20000,self.points,self.k,self.pointx,refinery = False)\n                    self.pointx=dep_new.prevent_duplicates() \n                    \n                    self.depots[action[0]] =dep_new\n                    self.dep_points[action[0]] =dep_new.point \n                    self.state = self.recalc() \n                else:\n                    reward -= 20\n                    \n            elif action[1] == 1:# right\n                if indx+10<=2417:\n                    for i in self.depots[action[0]].sources:\n                        self.pointx.append(i)\n                    dep_new = Component(indx+10,\"depot_location\",indx+10,20000,self.points,self.k,self.pointx,refinery = False)                    \n                    self.pointx=dep_new.prevent_duplicates() \n                    self.depots[action[0]] =dep_new\n                    self.dep_points[action[0]] =dep_new.point \n                    self.state = self.recalc() \n                else:\n                    reward -= 20\n                    \n                    \n                \n        \n        # Calculating the reward\n        if len(self.previous_score)>0:\n            reward += (20*(self.previous_score[-1]-self.state))/self.previous_score[-1]\n            self.previous_score.append(self.state)\n \n  \n        elif len(self.previous_score)==0:\n            self.previous_score.append(self.state) \n            \n             \n        \n        # Checking if episode is done\n        if self.episode_len <= 0: \n            done = True\n        else:\n            done = False\n        \n        # Setting the placeholder for info\n        info = {}\n        \n        # Returning the step information\n        return self.observe(), reward, done,info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Enviro = Environment(25,5)\nstate = Enviro.observation_space.shape\nactions = Enviro.action_space.n","metadata":{"execution":{"iopub.status.busy":"2023-08-13T00:00:49.303721Z","iopub.execute_input":"2023-08-13T00:00:49.304169Z","iopub.status.idle":"2023-08-13T00:00:52.102163Z","shell.execute_reply.started":"2023-08-13T00:00:49.304133Z","shell.execute_reply":"2023-08-13T00:00:52.100918Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"done\ndone\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rl-agents==0.1.1\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T13:58:31.613643Z","iopub.execute_input":"2023-08-12T13:58:31.614052Z","iopub.status.idle":"2023-08-12T13:58:47.686227Z","shell.execute_reply.started":"2023-08-12T13:58:31.614023Z","shell.execute_reply":"2023-08-12T13:58:47.685032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model\nmodel = tf.keras.models.Sequential()    \nmodel.add(tf.keras.layers.Dense(32, activation='relu', input_shape =(1,1)))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(actions, activation='linear'))\n","metadata":{"execution":{"iopub.status.busy":"2023-08-13T00:01:01.844479Z","iopub.execute_input":"2023-08-13T00:01:01.844853Z","iopub.status.idle":"2023-08-13T00:01:01.939214Z","shell.execute_reply.started":"2023-08-13T00:01:01.844822Z","shell.execute_reply":"2023-08-13T00:01:01.938237Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-12T13:51:12.393615Z","iopub.execute_input":"2023-08-12T13:51:12.394004Z","iopub.status.idle":"2023-08-12T13:51:12.412963Z","shell.execute_reply.started":"2023-08-12T13:51:12.393974Z","shell.execute_reply":"2023-08-12T13:51:12.412188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef build_agent(model, actions):\n    policy = BoltzmannQPolicy()\n    memory = SequentialMemory(limit=50000, window_length=1)\n    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n                  nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)\n    return dqn","metadata":{"execution":{"iopub.status.busy":"2023-08-12T23:51:06.533837Z","iopub.execute_input":"2023-08-12T23:51:06.534583Z","iopub.status.idle":"2023-08-12T23:51:06.540662Z","shell.execute_reply.started":"2023-08-12T23:51:06.534544Z","shell.execute_reply":"2023-08-12T23:51:06.539514Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"Agent = build_agent(model, actions)\nAgent.compile(tf.keras.optimizers.legacy.Adam(learning_rate=1e-2), metrics=['mae'])\nAgent.fit(Enviro , nb_steps=6000, visualize=False, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T00:01:16.574911Z","iopub.execute_input":"2023-08-13T00:01:16.575348Z","iopub.status.idle":"2023-08-13T00:03:53.081082Z","shell.execute_reply.started":"2023-08-13T00:01:16.575316Z","shell.execute_reply":"2023-08-13T00:03:53.079889Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Training for 6000 steps ...\ndone\ndone\n(1,)\nInterval 1 (0 steps performed)\n    1/10000 [..............................] - ETA: 30:02 - reward: 0.0000e+00","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n  updates=self.state_updates,\n","output_type":"stream"},{"name":"stdout","text":"    7/10000 [..............................] - ETA: 7:25 - reward: -4.6059<class '__main__.Component'>\n    9/10000 [..............................] - ETA: 6:51 - reward: -4.2475<class '__main__.Component'>\n   11/10000 [..............................] - ETA: 6:20 - reward: -3.8723<class '__main__.Component'>\n   14/10000 [..............................] - ETA: 6:16 - reward: -3.3549<class '__main__.Component'>\n   16/10000 [..............................] - ETA: 6:00 - reward: -3.0894<class '__main__.Component'>\n   23/10000 [..............................] - ETA: 6:05 - reward: -2.4115<class '__main__.Component'>\n<class '__main__.Component'>\n   26/10000 [..............................] - ETA: 5:41 - reward: -2.2097<class '__main__.Component'>\n   29/10000 [..............................] - ETA: 5:39 - reward: -2.0429<class '__main__.Component'>\n   34/10000 [..............................] - ETA: 5:38 - reward: -1.8220<class '__main__.Component'>\n   39/10000 [..............................] - ETA: 5:35 - reward: -1.6392<class '__main__.Component'>\n<class '__main__.Component'>\n   60/10000 [..............................] - ETA: 5:37 - reward: -1.1888<class '__main__.Component'>\n<class '__main__.Component'>\n   71/10000 [..............................] - ETA: 5:38 - reward: -1.0470<class '__main__.Component'>\n   79/10000 [..............................] - ETA: 5:43 - reward: -0.9726done\ndone\n(1,)\n   89/10000 [..............................] - ETA: 11:15 - reward: -1.2400<class '__main__.Component'>\n  101/10000 [..............................] - ETA: 10:42 - reward: -1.2198","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n  updates=self.state_updates,\n","output_type":"stream"},{"name":"stdout","text":"  104/10000 [..............................] - ETA: 11:54 - reward: -1.2084<class '__main__.Component'>\n  109/10000 [..............................] - ETA: 11:41 - reward: -1.1863<class '__main__.Component'>\n  128/10000 [..............................] - ETA: 11:08 - reward: -1.0904<class '__main__.Component'>\n  141/10000 [..............................] - ETA: 10:41 - reward: -1.0171<class '__main__.Component'>\n  143/10000 [..............................] - ETA: 10:36 - reward: -1.0073<class '__main__.Component'>\n  153/10000 [..............................] - ETA: 10:26 - reward: -0.9666<class '__main__.Component'>\n  160/10000 [..............................] - ETA: 10:15 - reward: -0.9342done\ndone\n(1,)\n  171/10000 [..............................] - ETA: 12:43 - reward: -1.1512<class '__main__.Component'>\n  192/10000 [..............................] - ETA: 12:11 - reward: -1.1279<class '__main__.Component'>\n  203/10000 [..............................] - ETA: 11:54 - reward: -1.0855<class '__main__.Component'>\n  212/10000 [..............................] - ETA: 11:42 - reward: -1.0530<class '__main__.Component'>\n  227/10000 [..............................] - ETA: 11:23 - reward: -1.0022<class '__main__.Component'>\n  237/10000 [..............................] - ETA: 11:10 - reward: -0.9696<class '__main__.Component'>\n  239/10000 [..............................] - ETA: 11:07 - reward: -0.9628done\ndone\n(1,)\n<class '__main__.Component'>\n  241/10000 [..............................] - ETA: 12:57 - reward: -0.9553<class '__main__.Component'>\n<class '__main__.Component'>\n  245/10000 [..............................] - ETA: 12:46 - reward: -1.0445<class '__main__.Component'>\n  254/10000 [..............................] - ETA: 12:31 - reward: -1.0806<class '__main__.Component'>\n  261/10000 [..............................] - ETA: 12:22 - reward: -1.0791<class '__main__.Component'>\n  263/10000 [..............................] - ETA: 12:18 - reward: -1.0773<class '__main__.Component'>\n  265/10000 [..............................] - ETA: 12:15 - reward: -1.0747<class '__main__.Component'>\n  276/10000 [..............................] - ETA: 12:02 - reward: -1.0565<class '__main__.Component'>\n<class '__main__.Component'>\n  292/10000 [..............................] - ETA: 11:59 - reward: -1.0411<class '__main__.Component'>\n<class '__main__.Component'>\n  307/10000 [..............................] - ETA: 11:47 - reward: -1.0151<class '__main__.Component'>\n  319/10000 [..............................] - ETA: 11:35 - reward: -0.9883done\ndone\n(1,)\n<class '__main__.Component'>\n  321/10000 [..............................] - ETA: 12:58 - reward: -0.9827<class '__main__.Component'>\n  338/10000 [>.............................] - ETA: 12:39 - reward: -1.0763<class '__main__.Component'>\n  349/10000 [>.............................] - ETA: 12:33 - reward: -1.0628<class '__main__.Component'>\n  355/10000 [>.............................] - ETA: 12:28 - reward: -1.0516<class '__main__.Component'>\n  360/10000 [>.............................] - ETA: 12:23 - reward: -1.0410<class '__main__.Component'>\n  368/10000 [>.............................] - ETA: 12:14 - reward: -1.0238<class '__main__.Component'>\n  384/10000 [>.............................] - ETA: 12:01 - reward: -1.0453<class '__main__.Component'>\n<class '__main__.Component'>\n  390/10000 [>.............................] - ETA: 11:53 - reward: -1.0334<class '__main__.Component'>\n  400/10000 [>.............................] - ETA: 11:43 - reward: -1.0135done\ndone\n(1,)\n<class '__main__.Component'>\n  401/10000 [>.............................] - ETA: 12:52 - reward: -1.0110<class '__main__.Component'>\n<class '__main__.Component'>\n  419/10000 [>.............................] - ETA: 12:38 - reward: -1.0820<class '__main__.Component'>\n  425/10000 [>.............................] - ETA: 12:34 - reward: -1.0796<class '__main__.Component'>\n  438/10000 [>.............................] - ETA: 12:25 - reward: -1.0679<class '__main__.Component'>\n  445/10000 [>.............................] - ETA: 12:20 - reward: -1.0572<class '__main__.Component'>\n  465/10000 [>.............................] - ETA: 12:06 - reward: -1.0211<class '__main__.Component'>\n  468/10000 [>.............................] - ETA: 12:04 - reward: -1.0157<class '__main__.Component'>\n  476/10000 [>.............................] - ETA: 11:58 - reward: -1.0013<class '__main__.Component'>\n  479/10000 [>.............................] - ETA: 11:55 - reward: -0.9959done\ndone\n(1,)\n  486/10000 [>.............................] - ETA: 12:46 - reward: -1.0409<class '__main__.Component'>\n  488/10000 [>.............................] - ETA: 12:45 - reward: -1.0463<class '__main__.Component'>\n  499/10000 [>.............................] - ETA: 12:37 - reward: -1.0915<class '__main__.Component'>\n  501/10000 [>.............................] - ETA: 12:36 - reward: -1.0917<class '__main__.Component'>\n  509/10000 [>.............................] - ETA: 12:30 - reward: -1.1230<class '__main__.Component'>\n  511/10000 [>.............................] - ETA: 12:28 - reward: -1.1198<class '__main__.Component'>\n  516/10000 [>.............................] - ETA: 12:25 - reward: -1.1120<class '__main__.Component'>\n  525/10000 [>.............................] - ETA: 12:20 - reward: -1.1016<class '__main__.Component'>\n  537/10000 [>.............................] - ETA: 12:14 - reward: -1.0904<class '__main__.Component'>\n<class '__main__.Component'>\n  545/10000 [>.............................] - ETA: 12:05 - reward: -1.1175<class '__main__.Component'>\n  547/10000 [>.............................] - ETA: 12:03 - reward: -1.1150<class '__main__.Component'>\n  559/10000 [>.............................] - ETA: 11:56 - reward: -1.0994done\ndone\n(1,)\n  567/10000 [>.............................] - ETA: 12:39 - reward: -1.1334<class '__main__.Component'>\n  577/10000 [>.............................] - ETA: 12:34 - reward: -1.1308<class '__main__.Component'>\n  585/10000 [>.............................] - ETA: 12:29 - reward: -1.1237<class '__main__.Component'>\n<class '__main__.Component'>\n  588/10000 [>.............................] - ETA: 12:26 - reward: -1.1205<class '__main__.Component'>\n  600/10000 [>.............................] - ETA: 12:19 - reward: -1.1088<class '__main__.Component'>\n  614/10000 [>.............................] - ETA: 12:11 - reward: -1.0947<class '__main__.Component'>\n  640/10000 [>.............................] - ETA: 12:00 - reward: -1.0664done\ndone\n(1,)\n  641/10000 [>.............................] - ETA: 12:40 - reward: -1.0647<class '__main__.Component'>\n<class '__main__.Component'>\n  645/10000 [>.............................] - ETA: 12:37 - reward: -1.0979<class '__main__.Component'>\n  657/10000 [>.............................] - ETA: 12:37 - reward: -1.1101<class '__main__.Component'>\n  719/10000 [=>............................] - ETA: 12:09 - reward: -1.0433<class '__main__.Component'>\ndone\ndone\n(1,)\n  724/10000 [=>............................] - ETA: 12:43 - reward: -1.0692<class '__main__.Component'>\n  735/10000 [=>............................] - ETA: 12:38 - reward: -1.0911<class '__main__.Component'>\n  744/10000 [=>............................] - ETA: 12:34 - reward: -1.0914<class '__main__.Component'>\n  746/10000 [=>............................] - ETA: 12:32 - reward: -1.0908<class '__main__.Component'>\n<class '__main__.Component'>\n  749/10000 [=>............................] - ETA: 12:30 - reward: -1.0895<class '__main__.Component'>\n  758/10000 [=>............................] - ETA: 12:26 - reward: -1.0841<class '__main__.Component'>\n  765/10000 [=>............................] - ETA: 12:22 - reward: -1.0788<class '__main__.Component'>\n  772/10000 [=>............................] - ETA: 12:18 - reward: -1.0731<class '__main__.Component'>\n  800/10000 [=>............................] - ETA: 12:05 - reward: -1.0464done\ndone\n(1,)\n  806/10000 [=>............................] - ETA: 12:35 - reward: -1.0747<class '__main__.Component'>\n  831/10000 [=>............................] - ETA: 12:24 - reward: -1.0823<class '__main__.Component'>\n  836/10000 [=>............................] - ETA: 12:22 - reward: -1.0787<class '__main__.Component'>\n  842/10000 [=>............................] - ETA: 12:19 - reward: -1.0743<class '__main__.Component'>\n  860/10000 [=>............................] - ETA: 12:11 - reward: -1.0599<class '__main__.Component'>\n  863/10000 [=>............................] - ETA: 12:10 - reward: -1.0575<class '__main__.Component'>\n  880/10000 [=>............................] - ETA: 12:03 - reward: -1.0442done\ndone\n(1,)\n  884/10000 [=>............................] - ETA: 12:30 - reward: -1.0634<class '__main__.Component'>\n  886/10000 [=>............................] - ETA: 12:29 - reward: -1.0695<class '__main__.Component'>\n  906/10000 [=>............................] - ETA: 12:21 - reward: -1.0735<class '__main__.Component'>\n  913/10000 [=>............................] - ETA: 12:18 - reward: -1.0690<class '__main__.Component'>\n  917/10000 [=>............................] - ETA: 12:16 - reward: -1.0662<class '__main__.Component'>\n  924/10000 [=>............................] - ETA: 12:13 - reward: -1.0613<class '__main__.Component'>\n  928/10000 [=>............................] - ETA: 12:11 - reward: -1.0583<class '__main__.Component'>\n<class '__main__.Component'>\n  931/10000 [=>............................] - ETA: 12:10 - reward: -1.0560<class '__main__.Component'>\n  938/10000 [=>............................] - ETA: 12:06 - reward: -1.0505<class '__main__.Component'>\n  940/10000 [=>............................] - ETA: 12:05 - reward: -1.0489<class '__main__.Component'>\n  949/10000 [=>............................] - ETA: 12:01 - reward: -1.0420<class '__main__.Component'>\n  953/10000 [=>............................] - ETA: 11:59 - reward: -1.0391<class '__main__.Component'>\n  959/10000 [=>............................] - ETA: 11:56 - reward: -1.0342<class '__main__.Component'>\ndone\ndone\n(1,)\n  993/10000 [=>............................] - ETA: 12:02 - reward: -1.5013<class '__main__.Component'>\n  999/10000 [=>............................] - ETA: 11:59 - reward: -1.5015<class '__main__.Component'>\n 1005/10000 [==>...........................] - ETA: 11:57 - reward: -1.4990<class '__main__.Component'>\n 1016/10000 [==>...........................] - ETA: 11:53 - reward: -1.4948<class '__main__.Component'>\n 1032/10000 [==>...........................] - ETA: 11:48 - reward: -1.4818<class '__main__.Component'>\n 1040/10000 [==>...........................] - ETA: 11:45 - reward: -1.4741done\ndone\n(1,)\n 1052/10000 [==>...........................] - ETA: 12:10 - reward: -1.4989<class '__main__.Component'>\n 1061/10000 [==>...........................] - ETA: 12:07 - reward: -1.5005<class '__main__.Component'>\n 1065/10000 [==>...........................] - ETA: 12:06 - reward: -1.4987<class '__main__.Component'>\n 1075/10000 [==>...........................] - ETA: 12:02 - reward: -1.4920<class '__main__.Component'>\n 1077/10000 [==>...........................] - ETA: 12:01 - reward: -1.4903<class '__main__.Component'>\n 1082/10000 [==>...........................] - ETA: 11:59 - reward: -1.4860<class '__main__.Component'>\n 1093/10000 [==>...........................] - ETA: 11:55 - reward: -1.4754<class '__main__.Component'>\n 1101/10000 [==>...........................] - ETA: 11:53 - reward: -1.4670<class '__main__.Component'>\n 1112/10000 [==>...........................] - ETA: 11:49 - reward: -1.4553<class '__main__.Component'>\n 1116/10000 [==>...........................] - ETA: 11:47 - reward: -1.4510<class '__main__.Component'>\n<class '__main__.Component'>\n 1120/10000 [==>...........................] - ETA: 11:45 - reward: -1.4467done\ndone\n(1,)\n 1126/10000 [==>...........................] - ETA: 12:04 - reward: -1.4651<class '__main__.Component'>\n 1185/10000 [==>...........................] - ETA: 11:44 - reward: -1.4269<class '__main__.Component'>\n 1200/10000 [==>...........................] - ETA: 11:40 - reward: -1.4117done\ndone\n(1,)\n 1210/10000 [==>...........................] - ETA: 11:58 - reward: -1.4317<class '__main__.Component'>\n 1218/10000 [==>...........................] - ETA: 11:56 - reward: -1.4312<class '__main__.Component'>\n 1221/10000 [==>...........................] - ETA: 11:55 - reward: -1.4296<class '__main__.Component'>\n<class '__main__.Component'>\n 1230/10000 [==>...........................] - ETA: 11:52 - reward: -1.4237<class '__main__.Component'>\n 1237/10000 [==>...........................] - ETA: 11:50 - reward: -1.4192<class '__main__.Component'>\n 1244/10000 [==>...........................] - ETA: 11:48 - reward: -1.4137<class '__main__.Component'>\n 1253/10000 [==>...........................] - ETA: 11:46 - reward: -1.4067<class '__main__.Component'>\n 1257/10000 [==>...........................] - ETA: 11:44 - reward: -1.4034<class '__main__.Component'>\n 1265/10000 [==>...........................] - ETA: 11:42 - reward: -1.3967<class '__main__.Component'>\n<class '__main__.Component'>\n 1278/10000 [==>...........................] - ETA: 11:38 - reward: -1.3856<class '__main__.Component'>\n<class '__main__.Component'>\ndone\ndone\n(1,)\n 1297/10000 [==>...........................] - ETA: 11:52 - reward: -1.4100<class '__main__.Component'>\n<class '__main__.Component'>\n 1306/10000 [==>...........................] - ETA: 11:49 - reward: -1.4089<class '__main__.Component'>\n 1331/10000 [==>...........................] - ETA: 11:42 - reward: -1.3931<class '__main__.Component'>\n 1335/10000 [===>..........................] - ETA: 11:40 - reward: -1.3899<class '__main__.Component'>\n 1345/10000 [===>..........................] - ETA: 11:37 - reward: -1.3818<class '__main__.Component'>\n 1348/10000 [===>..........................] - ETA: 11:36 - reward: -1.3794<class '__main__.Component'>\n 1354/10000 [===>..........................] - ETA: 11:34 - reward: -1.3745<class '__main__.Component'>\n 1360/10000 [===>..........................] - ETA: 11:32 - reward: -1.3692done\ndone\n(1,)\n<class '__main__.Component'>\n 1361/10000 [===>..........................] - ETA: 11:49 - reward: -1.3682<class '__main__.Component'>\n<class '__main__.Component'>\n 1367/10000 [===>..........................] - ETA: 11:47 - reward: -1.3838<class '__main__.Component'>\n 1381/10000 [===>..........................] - ETA: 11:43 - reward: -1.3864<class '__main__.Component'>\n 1389/10000 [===>..........................] - ETA: 11:41 - reward: -1.3827<class '__main__.Component'>\n<class '__main__.Component'>\n 1393/10000 [===>..........................] - ETA: 11:40 - reward: -1.3807<class '__main__.Component'>\n<class '__main__.Component'>\n 1397/10000 [===>..........................] - ETA: 11:38 - reward: -1.3784<class '__main__.Component'>\n 1402/10000 [===>..........................] - ETA: 11:37 - reward: -1.3753<class '__main__.Component'>\n 1414/10000 [===>..........................] - ETA: 11:33 - reward: -1.3677<class '__main__.Component'>\n 1420/10000 [===>..........................] - ETA: 11:32 - reward: -1.3634<class '__main__.Component'>\n 1433/10000 [===>..........................] - ETA: 11:28 - reward: -1.3534<class '__main__.Component'>\n 1438/10000 [===>..........................] - ETA: 11:26 - reward: -1.3493<class '__main__.Component'>\n 1440/10000 [===>..........................] - ETA: 11:26 - reward: -1.3477done\ndone\n(1,)\n 1459/10000 [===>..........................] - ETA: 11:42 - reward: -1.3788<class '__main__.Component'>\n<class '__main__.Component'>\n 1471/10000 [===>..........................] - ETA: 11:38 - reward: -1.3745<class '__main__.Component'>\n 1494/10000 [===>..........................] - ETA: 11:32 - reward: -1.3609<class '__main__.Component'>\n 1499/10000 [===>..........................] - ETA: 11:31 - reward: -1.3573<class '__main__.Component'>\n 1504/10000 [===>..........................] - ETA: 11:29 - reward: -1.3536<class '__main__.Component'>\n 1515/10000 [===>..........................] - ETA: 11:26 - reward: -1.3457<class '__main__.Component'>\n 1519/10000 [===>..........................] - ETA: 11:25 - reward: -1.3427done\ndone\n(1,)\n 1540/10000 [===>..........................] - ETA: 11:33 - reward: -1.4352<class '__main__.Component'>\n 1556/10000 [===>..........................] - ETA: 11:30 - reward: -1.4299<class '__main__.Component'>\n 1558/10000 [===>..........................] - ETA: 11:29 - reward: -1.4290<class '__main__.Component'>\n 1562/10000 [===>..........................] - ETA: 11:28 - reward: -1.4271<class '__main__.Component'>\n 1573/10000 [===>..........................] - ETA: 11:25 - reward: -1.4214<class '__main__.Component'>\n 1590/10000 [===>..........................] - ETA: 11:21 - reward: -1.4101<class '__main__.Component'>\n 1600/10000 [===>..........................] - ETA: 11:18 - reward: -1.4396done\ndone\n(1,)\n 1608/10000 [===>..........................] - ETA: 11:31 - reward: -1.4568<class '__main__.Component'>\n 1632/10000 [===>..........................] - ETA: 11:25 - reward: -1.4532<class '__main__.Component'>\n 1643/10000 [===>..........................] - ETA: 11:22 - reward: -1.4464<class '__main__.Component'>\n<class '__main__.Component'>\n<class '__main__.Component'>\n 1660/10000 [===>..........................] - ETA: 11:18 - reward: -1.4350<class '__main__.Component'>\n 1664/10000 [===>..........................] - ETA: 11:16 - reward: -1.4321<class '__main__.Component'>\n 1668/10000 [====>.........................] - ETA: 11:15 - reward: -1.4293<class '__main__.Component'>\n 1670/10000 [====>.........................] - ETA: 11:14 - reward: -1.4278<class '__main__.Component'>\n 1680/10000 [====>.........................] - ETA: 11:11 - reward: -1.4202done\ndone\n(1,)\n 1695/10000 [====>.........................] - ETA: 11:22 - reward: -1.4384<class '__main__.Component'>\n 1700/10000 [====>.........................] - ETA: 11:21 - reward: -1.4382<class '__main__.Component'>\n 1735/10000 [====>.........................] - ETA: 11:12 - reward: -1.4229<class '__main__.Component'>\n<class '__main__.Component'>\n 1758/10000 [====>.........................] - ETA: 11:06 - reward: -1.4093<class '__main__.Component'>\ndone\ndone\n(1,)\n 1761/10000 [====>.........................] - ETA: 11:17 - reward: -1.4072<class '__main__.Component'>\n 1795/10000 [====>.........................] - ETA: 11:09 - reward: -1.4168<class '__main__.Component'>\n 1806/10000 [====>.........................] - ETA: 11:07 - reward: -1.4125<class '__main__.Component'>\n 1815/10000 [====>.........................] - ETA: 11:06 - reward: -1.4082<class '__main__.Component'>\n 1824/10000 [====>.........................] - ETA: 11:05 - reward: -1.4036<class '__main__.Component'>\n 1840/10000 [====>.........................] - ETA: 11:02 - reward: -1.3942done\ndone\n(1,)\n 1858/10000 [====>.........................] - ETA: 11:10 - reward: -1.4103","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m Agent \u001b[38;5;241m=\u001b[39m build_agent(model, actions)\n\u001b[1;32m      2\u001b[0m Agent\u001b[38;5;241m.\u001b[39mcompile(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mlegacy\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[43mAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEnviro\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/rl/core.py:176\u001b[0m, in \u001b[0;36mAgent.fit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(action_repetition):\n\u001b[1;32m    175\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_action_begin(action)\n\u001b[0;32m--> 176\u001b[0m     observation, r, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     observation \u001b[38;5;241m=\u001b[39m deepcopy(observation)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","Cell \u001b[0;32mIn[7], line 104\u001b[0m, in \u001b[0;36mEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_len \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[1;32m    103\u001b[0m   \u001b[38;5;66;03m#selcting one the final motion [left,right]\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m dp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdep_points\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    105\u001b[0m indx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints\u001b[38;5;241m.\u001b[39mindex(dp)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m#left\u001b[39;00m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}]}]}